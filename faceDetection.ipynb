{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Face Detection Using OpenCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import get_ipython\n",
    "get_ipython().run_line_magic('run', 'main.ipynb') \n",
    "model = EmotionCNN()\n",
    "model.load_state_dict(torch.load('emotion_recognition_model.pth'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class EmotionCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EmotionCNN, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(128 * 6 * 6, 128) \n",
    "        self.fc2 = nn.Linear(128, 8) \n",
    "        \n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool1(torch.relu(self.conv1(x)))\n",
    "        x = self.pool2(torch.relu(self.conv2(x)))\n",
    "        x = self.pool3(torch.relu(self.conv3(x)))\n",
    "        x = x.view(-1, 128 * 6 * 6) \n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from IPython import get_ipython\n",
    "\n",
    "get_ipython().run_line_magic('run', 'main.ipynb')  \n",
    "\n",
    "model = EmotionCNN()\n",
    "model.load_state_dict(torch.load('emotion_recognition_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "emotions = ['angry','contempt' 'disgust', 'fear', 'neutral', 'happiness', 'sadness', 'surprise']\n",
    "\n",
    "img = cv2.imread('/home/shobhnikk/Documents/MusicProject/tryimage7.png')\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "if len(faces) > 0:\n",
    "    x, y, w, h = faces[0]\n",
    "    face = img[y:y+h, x:x+w]  \n",
    "\n",
    "    face_gray = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n",
    "    face_resized = cv2.resize(face_gray, (48, 48))\n",
    "\n",
    "    face_resized = face_resized.astype('float32') / 255  \n",
    "    face_resized = np.expand_dims(face_resized, axis=-1)  \n",
    "    face_resized = np.expand_dims(face_resized, axis=0)  \n",
    "\n",
    "    face_resized = torch.tensor(face_resized)  \n",
    "\n",
    "    face_resized = face_resized.squeeze(-1) \n",
    "    face_resized = face_resized.unsqueeze(1) \n",
    "\n",
    "    with torch.no_grad():  \n",
    "        emotion_prob = model(face_resized)\n",
    "        emotion = emotions[torch.argmax(emotion_prob)]  \n",
    "\n",
    "    print(f\"Predicted Emotion: {emotion}\")\n",
    "\n",
    "    cv2.rectangle(img, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "    cv2.imshow('Emotion Detection', img)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "else:\n",
    "    print(\"No face detected.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "csv_path = \"change it with the path of the csv file\"\n",
    "data = pd.read_csv(csv_path)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(),\n",
    "    transforms.Resize((48, 48)),  \n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "images = []\n",
    "labels = []\n",
    "for idx, row in data.iterrows():\n",
    "    image_path = row['image_path']\n",
    "    label = row['label'] \n",
    "    \n",
    "    image = Image.open(image_path)\n",
    "    image = transform(image)\n",
    "    images.append(image)\n",
    "    labels.append(label)\n",
    "\n",
    "images = torch.stack(images)\n",
    "if labels:  \n",
    "    labels = torch.tensor(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
